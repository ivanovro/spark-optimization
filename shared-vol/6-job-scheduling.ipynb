{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Scheduling\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the cluster mode, each Spark application runs an independent set of Executor processes. \n",
    "\n",
    "Cluster Manager Provides facilities for scheduling accross multiple Spark Applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jobs, Stages and Tasks\n",
    "\n",
    "Each job might consist of 1-N Stages, each stage might consist of 1-N Tasks.\n",
    "\n",
    "![jobs_stages_tasks](images/jobs_stages_tasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling\n",
    "\n",
    "The process of scheduling is providing access to the physical resources in the cluster.\n",
    "\n",
    "#### Slot\n",
    "In Spark there is concept of a Slot - minimum amount of resources needed to run a Task. Usually a Executor CPU == Slot.\n",
    "\n",
    "![tasks_executors_slots](images/tasks_executors_slots.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Resources Partitioning (Allocation)\n",
    "\n",
    "In most cluster implementations there is an option to fix the maximum amount of resources a Spark Applicaiton will use. It hold to this resources untill completion.\n",
    "\n",
    "Depending on the cluster implementation, resource allocation can be configured as:\n",
    "\n",
    "- Standalone Mode: by default applications run in FIFO order. Each application will try to use all available resources. You can limit the amount of resources an application might use.\n",
    "- Mesos: configure `spark.mesos.coarse=true`, `spark.cores.max` and `spark.executor.memory`\n",
    "- YARN: `spark.executor.instances`, `spark.executor.memory`, `spark.executor.cores`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Resources Allocation\n",
    "\n",
    "In Mesos, YARN and Kubernetes cluster managers it is possible to configure dynamic resources allocation occupied by an application.\n",
    "\n",
    "Dynamic Allocation means that depending on the current state of the Tasks Queue, an application either will get more resources or it will give the resources back.\n",
    "\n",
    "#### Request Policy\n",
    "\n",
    "Spark requests resources in rounds with exponential growth based on the number of tasks in the previous request round.\n",
    "\n",
    "The actual request is triggered when there have been pending tasks for `spark.dynamicAllocation.schedulerBacklogTimeout` seconds, and then triggered again every `spark.dynamicAllocation.sustainedSchedulerBacklogTimeout` seconds thereafter if the queue of pending tasks persists. \n",
    "\n",
    "Additionally, the number of executors requested in each round increases exponentially from the previous round. For instance, an application will add 1 executor in the first round, and then 2, 4, 8 and so on executors in the subsequent rounds.\n",
    "\n",
    "#### Remove Policy\n",
    "\n",
    "A Spark application removes an executor when it has been idle for more than `spark.dynamicAllocation.executorIdleTimeout` seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling Within a Spark Application\n",
    "\n",
    "Inside a given Spark Application (SparkContext instance), multiple parallel Jobs can run simultaneously if they were submitted from separate threads. For example network serving application.\n",
    "\n",
    "#### FIFO Scheduler\n",
    "First-In-First-Out schedules jobs in a sequential manner. First job submitted will get access to a set of resources and use them until completion. Then next submitted job will get access to resources.  \n",
    "`spark.scheduler.mode=FIFO`\n",
    "\n",
    "#### FAIR Scheduler\n",
    "Under fair sharing, Spark assigns tasks between jobs in a “round robin” fashion, so that all jobs get a roughly equal share of cluster resources. Short jobs submitted while a long job is running can start receiving resources right away and still get good response times, without waiting for the long job to finish.  \n",
    "`spark.scheduler.mode=FAIR`\n",
    "\n",
    "#### Pools\n",
    "The fair scheduler also supports grouping jobs into pools, and setting different scheduling options (e.g. weight) for each pool.  \n",
    "`sc.setLocalProperty(\"spark.scheduler.pool\", \"pool1\")`\n",
    "\n",
    "By default, each pool gets an equal share of the cluster (also equal in share to each job in the default pool), but inside each pool, jobs run in FIFO order. \n",
    "\n",
    "##### Configuration\n",
    "```\n",
    "<?xml version=\"1.0\"?>\n",
    "<allocations>\n",
    "  <pool name=\"production\">\n",
    "    <schedulingMode>FAIR</schedulingMode>\n",
    "    <weight>1</weight>\n",
    "    <minShare>2</minShare>\n",
    "  </pool>\n",
    "  <pool name=\"test\">\n",
    "    <schedulingMode>FIFO</schedulingMode>\n",
    "    <weight>2</weight>\n",
    "    <minShare>3</minShare>\n",
    "  </pool>\n",
    "</allocations>\n",
    "```\n",
    "\n",
    "- **schedulingMode**: This can be FIFO or FAIR, to control whether jobs within the pool queue up behind each other (the default) or share the pool’s resources fairly.\n",
    "- **weight**: This controls the pool’s share of the cluster relative to other pools. By default, all pools have a weight of 1. If you give a specific pool a weight of 2, for example, it will get 2x more resources as other active pools. Setting a high weight such as 1000 also makes it possible to implement priority between pools—in essence, the weight-1000 pool will always get to launch tasks first whenever it has jobs active.\n",
    "- **minShare**: Apart from an overall weight, each pool can be given a minimum shares (as a number of CPU cores) that the administrator would like it to have. The fair scheduler always attempts to meet all active pools’ minimum shares before redistributing extra resources according to the weights. The minShare property can, therefore, be another way to ensure that a pool can always get up to a certain number of resources (e.g. 10 cores) quickly without giving it a high priority for the rest of the cluster. By default, each pool’s minShare is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
