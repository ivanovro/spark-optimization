{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark basics\n",
    "\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "app_name = \"spark-3\"\n",
    "master   = \"spark://spark-master:7077\"\n",
    "data_dir = 'data/weather'\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "import pyspark.sql.functions as sf\n",
    "import pyspark.sql.types as st\n",
    "\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .appName(app_name)\n",
    "    .master(master) \n",
    "    .getOrCreate()\n",
    ")\n",
    "spark\n",
    "\n",
    "def generate_data(row_count=100):\n",
    "    counter = 0\n",
    "    while counter < row_count:\n",
    "        yield [random.randint(0, 300),''.join(random.choice('abcdefghijklmnopqrstuvwxyz') for i in range(10))]\n",
    "        counter += 1\n",
    "\n",
    "def count_rows_in_partitions(df):\n",
    "    def count_in_a_partition(iterator):\n",
    "      yield sum(1 for _ in iterator)\n",
    "\n",
    "    return df.rdd.mapPartitions(count_in_a_partition).collect()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "- Partitions\n",
    "- The anatomy of a Spark job\n",
    "- Narrow and wide transformation (and why you should care).\n",
    "- How Spark reads data (from HDFS).\n",
    "- The Catalyst optimizer.\n",
    "- Caching and persistence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partitions\n",
    "\n",
    "In order to allow for distributed computing, the data needs to be split up in some way. We call this partitioning. Spark has some built-in ways of controlling the partitioning:\n",
    "\n",
    " - Changing to a fixed number of partitions.\n",
    " - Hash-based distribution across partitions.\n",
    " - Range-based distribution across partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before we dive into partitioning, note that we already saw (the effects of) partitioning before.\n",
    "<img src=\"images/partitioning.png\" width=\"70%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Moreover, when reading from data formats like Parquet, you automatically get partitions, as they were stored on disk.\n",
    "\n",
    "For all the partitioning methods mentioned, if you do not specify a number of partitions, Spark will default to whatever has been configured in `spark.default.parallelism`. This config setting is set to 200 by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sdf1 = spark.createDataFrame(generate_data(row_count=1000), schema=['number', 'letter'])\n",
    "sdf1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rows_in_partitions(sdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Why do we end up with 2 partitions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When we conjure up a new DataFrame, Spark uses the `defaultParallelim` setting from the underlying Spark Context, which in turn is configured from the `spark.default.parallelism` setting, whose default depends on the cluster mode. In our mode local mode this means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.sparkContext.master, spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some DataFrame transformations implicitly repartition the data. When this happens the value of `spark.sql.shuffle.partitions` (default: 200) determines the number of partitions from that point onwards.\n",
    "\n",
    "The older RDD-based API had different rules; beware that the documentation and a lot of Internet information was incorrect about how this worked. If you find yourself needing to understand this, test and verify for your situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Controlling Partition Count\n",
    "\n",
    "In the last module we saw the `.partition()` transformation: this sets a fixed number of partitions from this point onwards until another transformation implictly changes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_part = sdf1.repartition(5)\n",
    "print(num_part.rdd.getNumPartitions())\n",
    "print(count_rows_in_partitions(num_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Upsides:\n",
    "\n",
    "+ Very simple.\n",
    "\n",
    "Downsides:\n",
    "\n",
    "+ You haven't specified how the data will be distributed across the partitions.\n",
    "+ It can be hard to choose a good number. (Spark can't choose one for you; it's a hard problem.)\n",
    "\n",
    "   - Too many partitions, and the overhead of an action goes up: lots of tasks doing not very much.\n",
    "   - Too few partitions, and the task may have \"too much\" work to and run out of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Range Partitioning\n",
    "\n",
    "Next to the number of partitions, we can control how the data is distributed across them. Range partitioning:\n",
    "\n",
    " - Orders the dataframe by the columns given.\n",
    " - Allocates the data based on ranges of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "range_part = sdf1.repartitionByRange(sf.col(\"number\"))\n",
    "print(range_part.rdd.getNumPartitions())\n",
    "print(count_rows_in_partitions(range_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_part_letter = sdf1.repartitionByRange(sf.col(\"letter\"))\n",
    "print(range_part_letter.rdd.getNumPartitions())\n",
    "print(count_rows_in_partitions(range_part_letter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Upsides:\n",
    "\n",
    "+ Relatively easy.\n",
    "\n",
    "Downsides:\n",
    "\n",
    "+ You need to know your data quite well.\n",
    "+ Duplicate keys can cause skew, making 1 range very popular, and thus the partition very large\n",
    "+ In order to determine the ranges, Spark needs to sample your data. This actually means evaluating the DataFrame/RDD. Therefore, repartitioning by range is actually a transformation __and__ an action.\n",
    "+ Each record in the dataframe will be shuffled to the right partition based on the range of the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hash Partitioning\n",
    "\n",
    "The normal partitioning is hash-based. Hashing column values _hopefully_ produces a uniform distribution without any sampling. This is the default mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hash_part = sdf1.repartition(sf.col(\"number\"))\n",
    "print(hash_part.rdd.getNumPartitions())\n",
    "# print(count_rows_in_partitions(hash_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_part_letter = sdf1.repartition(10, sf.col(\"letter\"))\n",
    "print(hash_part_letter.rdd.getNumPartitions())\n",
    "# print(count_rows_in_partitions(hash_part_letter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Upsides:\n",
    "+ Works out of the box, without complex configuration.\n",
    "\n",
    "Downsides:\n",
    "+ You need to pick your partition key wisely, or you may end up with very uneven distribution of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Partition Skew\n",
    "Each type of partitioning has its own advantages and disadvantages. Dealing with skewed partitions can be very challenging. A few general tips:\n",
    "+ If you have skewed partitions, try to figure out what in your data is causing it. In some cases, adding an extra column to partition on can help you out.\n",
    "+ Consider if you can hardcode the number of partitions. Note that this may cause downstream operations to require huge amounts of shuffling as the data will be spread randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pro-Tip\n",
    "\n",
    "_Salting_ is a technique for reducing partition skew:\n",
    "\n",
    "1. Add a column containing a random number.\n",
    "2. Include it in your set of columns to repartition.\n",
    "3. Drop it after the repartition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_salty = sdf1 \\\n",
    "   .withColumn(\"salt\", sf.rand(seed=0)) \\\n",
    "   .repartition(sf.col(\"salt\"), sf.col(\"number\")) \\\n",
    "   .drop(\"salt\")\n",
    "\n",
    "print(hash_salty.rdd.getNumPartitions())\n",
    "print(count_rows_in_partitions(hash_salty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Anatomy of a Spark Job\n",
    "+ __Application__: A `SparkSession` or `SparkContext` object\n",
    "+ __Job__: Each action creates a Spark *job*\n",
    "+ __Stage__: Each stage represents one wide transformation (i.e. every wide transformation introduces a new stage\n",
    "+ __Task__: One parallellizable unit of computation. There is one task per partition per stage.\n",
    "<center><img src=\"images/spark-job-anatomy.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Narrow and wide operations (and why you should care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a `filter` operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/narrow_op.svg\" width=\"70%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called a **narrow** operation, as there is no shuffling between the workers. Spark reserves the right to combine subsequent narrow operations together.\n",
    "\n",
    "It means that in case we do a `select` afterwards multiplying the second column by 2, Spark would not do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/narrow_op_two.svg\" width=\"80%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "but this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/pipelining.svg\" width=\"70%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above is a **fundamental** difference compared to Hadoop, where subsequent Map (and Reduce) operations always needed to write to disk. Instead, Spark combines the two operations in memory, resulting in much higher efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise narrow transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .appName(app_name)\n",
    "    .master(master) \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "import random\n",
    "sdf1 = spark.createDataFrame(generate_data(row_count=1000), schema=['number', 'letter'])\n",
    "\n",
    "sdf1.filter('number > 4').select('letter').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Open [Spark UI](http://localhost:4040/stages/)* and check how many stages you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If, on the other hand, we have an operation such as `groupBy`, we need to shuffle between machines. This is called a **wide** operation, and Spark cannot combine them together like **narrow** operations. For example (note that we call the two machines on the right **D** and **E** because they could be completely different workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/groupBy.svg\" width=\"80%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .appName(app_name)\n",
    "    .master(master) \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "import random\n",
    "sdf1 = spark.createDataFrame(generate_data(row_count=1000), schema=['number', 'letter'])\n",
    "\n",
    "array = sdf1.groupBy('number').agg(sf.mean('number')).filter('number > 5').collect()\n",
    "print(len(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Open [Spark UI](http://localhost:4040/stages/)* and check how many stages you have now.  \n",
    "Note partitions count for stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Spark reads data (from HDFS)\n",
    "\n",
    "First of all we should take a look at how HDFS store the data. In principle it works like this:\n",
    "\n",
    "![hdfs](images/hdfs.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When you read `Lorem.txt` in Spark, each operation on a **block** becomes a **task**. The larger the file, the more blocks you have, the higher number of Spark tasks you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/spark_count.svg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Catalyst optimizer\n",
    "\n",
    "People could write books about the Catalyst optimizer, but the concept is as follows: Spark can not only chain narrow operations together, but also shuffle the operations order. Let's assume we have the following code (don't worry if you don't grok the API yet):\n",
    "\n",
    "```python\n",
    "(\n",
    "    df.groupBy('name')\n",
    "      .agg(sf.mean('amount').alias('avg_amount'))\n",
    "      .filter(sf.col('name') == 'name_1')\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is a very stupid thing to do, because we should really write this:\n",
    "\n",
    "```python\n",
    "(\n",
    "    df.filter(sf.col('name') == 'name_1')\n",
    "      .groupBy('name')\n",
    "      .agg(sf.mean('amount').alias('avg_amount'))\n",
    ")\n",
    "```\n",
    "Luckily the Catalyst optimizer takes care of the above shuffling by itself, resulting in quicker execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check [Spark UI](http://localhost:4040/SQL/)* for the previous query we've executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The two most common things we talk about are:\n",
    "\n",
    " - Predicate pushdown;\n",
    " - Projection pushdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These are both ways of discarding data as early in the process as possible. Spark can even push them down to the data-sources that are reading data:\n",
    "\n",
    " - Parquet in particular will only read columns that are needed (projection pushdown);\n",
    " - The latest Avro source will skip the rest of a record if it determines it is being filtered (predicate pushdown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many more optimisations are supported, and these also end up combination with specialized execution strategies for common combinations of operation.\n",
    "\n",
    "The exact optimizations are outside the scope of this training, but if you look into it you'll discover that you can implement and plug in your own optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![catalyst](images/catalyst.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Cost-based optimization relies on table/column statistics, which aren't always available. So more often you only see rule-based optimizations. Note that sometimes you may wish to disable the cost-based optimizer: it can be unpredictable and in production sometimes slower but more predictable is what you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SparkUI\n",
    "Detailed overview of your running job\n",
    "- Console prints the URL when starting up Spark.\n",
    "- Each spark-shell/spark application will start a new SparkUI with a higher port number.\n",
    "- The default (first) SparkUI http://localhost:4040\n",
    "\n",
    "![SparkUI](images/sparkUI-jobs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![SparkUI](images/SparkUI-stages.png)\n",
    "- Spark action are executed through multiple stages.\n",
    "- Track the progress of the stages.\n",
    "- View your accumulators by clicking on the description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![SparkUI](images/SparkUI-storage.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![SparkUI](images/SparkUI-executors.png)\n",
    "- Spark can run on multiple executors.\n",
    "- All executors are listed here.\n",
    "- You find the logs of the executors here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Caching and Persistence\n",
    "\n",
    "What happens when you call `count`, then `filter`, and then `count` again?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Calling `count` again will execute all the transformations (including reading from file) twice!!\n",
    "\n",
    "Persisting a DataFrame to memory will cache the data to avoid recomputing.\n",
    "\n",
    "This also helps in algorithms which execute functions iterative on a DataFrame like KMeans for example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![persistence](images/persistence.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `cache()`: Persists DataFrame with the default storage level\n",
    "\n",
    "Tell Spark to (temporarily) store the results so it doesn't have to recompute.\n",
    "\n",
    "Note:\n",
    "- You'll keep all resources allocated to you so far.\n",
    "You could permanently hog all the resources if you share a cluster.\n",
    "- Keeping stuff in memory means less room for computations.\n",
    "You may get weird Spark errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sdf1 = spark.createDataFrame([[1, 'a'], [2, 'b'], [2,  'c'], [3, 'd']],\n",
    "                             schema=['number', 'letter'])\n",
    "sdf1.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Stop! Go check the [Spark UI](http://localhost:4040/storage/), tab *Storage*. I'll wait here! What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wait, Spark is lying to us! We asked it to `cache` but there's nothing in storage! The reason is that `cache` is not an action, and Spark is lazy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Try invoking `count()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.unpersist()\n",
    "sdf1.persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "sdf1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the Spark UI. What do you see now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is important to note that **you need to trigger an action, i.e. a computation** in order to cache your DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/caching.svg\" width=\"80%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But where are we caching? By default calling `.cache` will cache a DataFrame in memory. There are other options accessible via the `.persist` method, namely:\n",
    "\n",
    "- `MEMORY_ONLY`\n",
    "- `DISK_ONLY`\n",
    "- `MEMORY_AND_DISK`\n",
    "- `MEMORY_ONLY_SER`\n",
    "- `MEMORY_AND_DISK_SER`\n",
    "- `MEMORY_ONLY_2`\n",
    "- `DISK_ONLY_2`\n",
    "- `MEMORY_AND_DISK_2`\n",
    "- `MEMORY_ONLY_SER_2`\n",
    "- `MEMORY_AND_DISK_SER_2`\n",
    "- `OFF_HEAP`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `MEMORY_ONLY` is the default;\n",
    "- `.cache()` is the same as `persist(MEMORY_ONLY)`\n",
    "- `_SER` means store in Serialized (binary) form;\n",
    "- `_2` means replicate the storage over 2 executor nodes;\n",
    "- `OFF_HEAP` is experimental feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this chapter we learned about:\n",
    "- What components are in a Spark Job\n",
    "- Narrow and wide operations (and why you should care);\n",
    "- How Spark reads data (from HDFS);\n",
    "- The Catalyst optimizer;\n",
    "- Caching and persistence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Exercise - Catalyst advanced\n",
    "\n",
    "For more detail on how Catalyst works, read the following blog on a deep dive into the Catalyst optimizer:\n",
    "https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html. \n",
    "\n",
    "If you're feeling more adventurous, you can also read the accompanying paper: http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf.\n",
    "\n",
    "After reading the material, see if you can answer the following questions:\n",
    "\n",
    "1. How does Catalyst represent your computation interally? What is the main advantage of this representation?\n",
    "2. Can you draw this representation for the Heroes computation from the previous exercise?\n",
    "3. Can you explain what the below optimizations do?\n",
    "    * Constant folding\n",
    "    * Predicate pushdown\n",
    "    * Projection pruning\n",
    "    * Null propagation\n",
    "    * Boolean expression simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
