{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "master = 'local[2]'\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .master(master) \n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. DataFrames: Windows\n",
    "\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Windows are analytical functions that work on parts of your DataFrame.\n",
    "\n",
    "- Saves you from groupby-agg-join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll try it with our heroes.\n",
    "\n",
    "__Goal__: compute attack de-meaned per role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heroes_path = os.path.join(data_dir, 'heroes.csv')\n",
    "heroes = spark.read.csv(heroes_path, header=True, nanValue='NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First compute the average attack per role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_per_role = (\n",
    "    heroes\n",
    "    .groupBy('role')\n",
    "    .agg(sf.mean('attack').alias('avg_attack'))\n",
    ")\n",
    "attack_per_role.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Join it back on and substract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groupby_heroes = (\n",
    "    heroes\n",
    "    .join(attack_per_role, on = ['role'])\n",
    "    .withColumn('demeaned_attack', sf.col('attack') - sf.col('avg_attack'))\n",
    ")\n",
    "groupby_heroes.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Doing it all at once with a window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "role_window = Window.partitionBy('role')\n",
    "window_heroes = (\n",
    "    heroes\n",
    "    .withColumn('demeaned_attack', \n",
    "                sf.col('attack') - sf.mean('attack').over(role_window))\n",
    ")\n",
    "window_heroes.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Question__: Are the groupby-join and window approaches the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The results are the same, but the ordering of the columns (and rows) is different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('window columns:', window_heroes.columns)\n",
    "print('groupby columns:', groupby_heroes.columns)\n",
    "print('\\nwindow first:\\n', window_heroes.select('name').limit(2).toPandas())\n",
    "print('\\ngroupby first:\\n', groupby_heroes.select('name').limit(2).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How did we do it?\n",
    "\n",
    "1. Define a window\n",
    "    - Partitions\n",
    "    - Ordering within partitions (not used above)\n",
    "    - Window size going over ordered partitions (not used above)\n",
    "1. Execute with an analytical function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame([['first', 2.0, 3],\n",
    "                             ['first', 1.5, 2],\n",
    "                             ['first', 3.0, 5],\n",
    "                             ['second', 0.5, 1],\n",
    "                             ['second', 1.0, 2]],\n",
    "                            ['a', 'b', 'c'])\n",
    "window = (\n",
    "    Window\n",
    "    .partitionBy('a')  # Frames.\n",
    "    .orderBy('b')  # Ordering within frames.\n",
    ")  # No window size specified.\n",
    "\n",
    "window2 = Window.partitionBy('a')\n",
    "\n",
    "(\n",
    "    sdf\n",
    "    .withColumn('XXX', sf.sum('c').over(window))  # Analytical function.\n",
    "    .show()\n",
    ")\n",
    "\n",
    "(\n",
    "    sdf\n",
    "    .withColumn('XXX', sf.sum('c').over(window2))  # Analytical function.\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll illustrate windows on a new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = spark.createDataFrame([[1, 1.0, 3.0],\n",
    "                                      [2, 1.0, 6.0],\n",
    "                                      [3, 2.0, 4.0],\n",
    "                                      [4, 3.0, 8.0],\n",
    "                                      [5, 3.0, 9.0],\n",
    "                                      [6, 3.0, 8.0],\n",
    "                                      [7, 3.0, 12.0]], \n",
    "                                     schema=['mid', 'month', 'temperature'])\n",
    "temperatures.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### `Window.rowsBetween()`: Window size\n",
    "\n",
    "Specify size of window in terms of rows before and after a row in ordering.\n",
    "\n",
    "<img src=\"images/rolling_window.png\" width=\"60%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mid_window = (\n",
    "    Window  # No partitions: full DataFrame\n",
    "    .orderBy('mid')  # Ordered by mid\n",
    "    .rowsBetween(-1, 0)  # Moving window of size 1\n",
    ")\n",
    "(\n",
    "    temperatures\n",
    "     .withColumn('window_start', sf.first('mid').over(mid_window))\n",
    "     .withColumn('window_end', sf.last('mid').over(mid_window))\n",
    "     .withColumn('mean_temp', sf.mean('temperature').over(mid_window))\n",
    "     .sort('mid')\n",
    "     .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or use an expanding window:\n",
    "\n",
    "<img src=\"images/expanding_window.png\" width=\"60%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "expanding_window = Window.orderBy('mid').rowsBetween(-sys.maxsize, 0)\n",
    "(\n",
    "    temperatures\n",
    "     .withColumn('window_start', sf.first('mid').over(expanding_window))\n",
    "     .withColumn('window_end', sf.last('mid').over(expanding_window))\n",
    "     .withColumn('mean_temp', sf.mean('temperature').over(expanding_window))\n",
    "     .sort('mid')\n",
    "     .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`.orderBy(col)` without row specification is implicitly `.orderBy(col).rowsBetween(-sys.maxsize, 0)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_window = Window.orderBy('mid')\n",
    "(\n",
    "    temperatures\n",
    "     .withColumn('window_start', sf.first('mid').over(alternative_window))\n",
    "     .withColumn('window_end', sf.last('mid').over(alternative_window))\n",
    "     .withColumn('mean_temp', sf.mean('temperature').over(alternative_window))\n",
    "     .sort('mid')\n",
    "     .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rolling mean across months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_window = Window.orderBy('month')\n",
    "(\n",
    "    temperatures\n",
    "    .withColumn('window_start', sf.first('mid').over(month_window))\n",
    "    .withColumn('window_end', sf.last('mid').over(month_window))\n",
    "    .withColumn('mean_temp', sf.mean('temperature').over(month_window))\n",
    "    .sort('mid')\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`.partitionBy` followed by `.orderBy`: rolling means within each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window = Window.partitionBy('month').orderBy('mid')\n",
    "(\n",
    "    temperatures\n",
    "    .withColumn('window_start', sf.first('mid').over(rolling_window))\n",
    "    .withColumn('window_end', sf.last('mid').over(rolling_window))\n",
    "    .withColumn('mean_temp', sf.mean('temperature').over(rolling_window))\n",
    "    .sort('mid')\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `Window.rangeBetween()`: Window size on values\n",
    "\n",
    "- `.rowsBetween()`: this row and the next row\n",
    "- `.rangeBetween()`: this value and the next value (of the column specified in orderBy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/rangebetween.png\" width=\"80%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: The range is specified _relative_ to the value in 'this' row.\n",
    "range_window = Window.orderBy('month').rangeBetween(0, 1)\n",
    "(\n",
    "    temperatures\n",
    "    .withColumn('window_start_mid', sf.first('mid').over(range_window))\n",
    "    .withColumn('window_end_mid', sf.last('mid').over(range_window))\n",
    "    .withColumn('max_temp', sf.max('temperature').over(range_window))\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## 6. Intermezzo: Comments about the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Spark is a powerful framework, but the PySpark API can be confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Multiple ways to do the same things:\n",
    "    - `sdf.filter()` vs `sdf.where()`\n",
    "    - `sf.isnull('name)` vs `sf.col('name').isNull()`\n",
    "    - `sf.col('a')` vs `sdf.a`\n",
    "- Functionality hidden in Spark functions.\n",
    "- Inconsistencies:\n",
    "    - Many functions accept a 'string' or `sf.col`, but some don't.\n",
    "    - Many functions accept many arguments or a list with arguments, but some don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You'll have to deal with it! ;-)\n",
    "\n",
    "Try to be consistent in how you write your Spark code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Windows are a powerful mechanism to apply aggregations or transformation over (parts of) your dataframe.\n",
    "\n",
    "A window function calculates a return value for every input row of a table based on a group of rows, called the Frame. Every input row can have a unique frame associated with it\n",
    "\n",
    "It also allows you to create rankings within groups of rows (e.g. top selling product per category etc.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Windows\n",
    "\n",
    "#### `temperatures`\n",
    "\n",
    "1. Add a column with the average temperature of the month.\n",
    "1. Compute the temperature delta with the previous measurement (hint: look what `sf.lag()` and `sf.lead()` do).\n",
    "1. Exclude rows of months with an average temperature below 5 degrees \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%load ../answers/02_windows_temperatures.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Windows\n",
    "\n",
    "1. Demean the flight delays in `arr_delay` partitioning by year (ignore missing and NaNs);\n",
    "1. Demean the flight delays partitioning by year/carrier;\n",
    "1. For each year, find the top 5 carriers with the most flights cancelled (hint: check if you should either use `sf.rank()` or `sf.rownumber()`;\n",
    "1. Same as previous, but rank by cancelled divided by the total number of flights per carrier/year;\n",
    "1. Per airline, find the airport with the most delays due to security reasons in a given year/month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_path = os.path.join(data_dir, \"airlines.parquet\")\n",
    "airlines = spark.read.parquet(airlines_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%load ../answers/02_windows_airlines.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Advanced windows\n",
    "\n",
    "1. Create a new column for a numeric column of your choice with the column difference between with the previous month in the `'airport', 'carrier'` window. When there is nothing in the previous element, put a 0 (not mathematically correct, but still).\n",
    "1. Remove all the groups of `'airport', 'carrier', 'year'` where more than 20% of flights is delayed by 2000 (make the parameters adjustable!)\n",
    "1. Take a look at the NA patterns in the `airlines` DataFrame. It seems like low volume airports do not have flights every month! Let's do something barbaric then: fill all the NA columns with the minimum historically known (i.e. expanding window) with partitions of `'airport', 'carrier'` ordered by `'year', 'month'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%load ../answers/02_windows_advanced.py"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
