{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Advanced\n",
    "\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "- Spark Memory Model\n",
    "- Java and Cryo Serializers\n",
    "- Garbage Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Memory Model\n",
    "\n",
    "Unlike Hadoop, Spark applications are memory heavy.  \n",
    "That's why understanding how memory works is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Worker and Executor memory configs\n",
    "\n",
    "`SPARK_WORKER_MEMORY=6g` - amount of memory available to a Worker node  \n",
    "`spark.executor.memory=4g` - amount of memory available to an Executor process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executor Memory structure\n",
    "Assuming we have an Executor with 4GB and memory settings are default. This is how the memory fractions will look like.\n",
    "\n",
    "![spark-memory](images/spark-memory.png)\n",
    "\n",
    "When it comes to memory optimization, usually these properties are tuned:\n",
    "\n",
    "- `spark.memory.fraction` — defaults to 0.75\n",
    "- `spark.memory.storageFraction` — defaults to 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reserved Memory fraction\n",
    "\n",
    "This region is dedicated to Spark internal objects - System.  \n",
    "Things like classes, services, network connections etc.\n",
    "\n",
    "It is hardcoded to be always **300MB**. Doesn't matter what is the size of other regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Memory fraction\n",
    "\n",
    "Stores user defined structures, like UDFs and other functions.  \n",
    "This region is not managed by Spark.\n",
    "\n",
    "Formula: `(JVM Heap - 300MB) * (1 - spark.memory.fraction)`\n",
    "\n",
    "In case of 4GB is `(4096MB - 300MB) * 0.25 = 949MB`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Memory\n",
    "\n",
    "Managed by Spark, used for storing intermediate state, computations, serilization, joins, broadcast variables, etc.  \n",
    "Caching, persisting in memory will be stored in the **storage** segment of this region.\n",
    "\n",
    "Formula : `(JVM Heap — Reserved Memory) * spark.memory.fraction`  \n",
    "\n",
    "In case of 4GB is `(4096MB -300MB) * 0.75 = 2847MB`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage Memory\n",
    "\n",
    "Is used for storing all caching and broadcasting data. \n",
    "\n",
    "Persistence options from the list use Storage Memory:\n",
    "- MEMORY_ONLY\n",
    "- MEMORY_AND_DISK\n",
    "- MEMORY_ONLY_SER\n",
    "- MEMORY_AND_DISK_SER\n",
    "- MEMORY_ONLY_2\n",
    "- MEMORY_AND_DISK_2\n",
    "- MEMORY_ONLY_SER_2\n",
    "- MEMORY_AND_DISK_SER_2\n",
    "\n",
    "Broadcast for example uses MEMORY_AND_DISK persistence option.\n",
    "\n",
    "Storage Memory works in the LRU (Least Recently Used) mode.  \n",
    "New data will be kept in memory and older will be evicted, to the disk or removed for the query plan recomutation.\n",
    "\n",
    "Formula: `(Java Heap — Reserved Memory) * spark.memory.fraction * spark.memory.storageFraction`\n",
    "\n",
    "In case of 4GB is `(4096MB — 300MB) * 0.75 * 0.5 = 1423MB`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Memory\n",
    "\n",
    "This segment is used for storing objects which are relevant to the Task execution.\n",
    "\n",
    "For example:\n",
    "- aggregations\n",
    "- shuffle intermediate buffer\n",
    "- serialization/deserialization\n",
    "\n",
    "This segment also supports spilling to disk, when there are not enough memory in the buffer.\n",
    "\n",
    "Is not LRU type of memory, tasks do not evict each other's memory.\n",
    "\n",
    "Formula: `(Java Heap — Reserved Memory) * spark.memory.fraction * (1.0 — spark.memory.storageFraction)`\n",
    "\n",
    "In case of 4GB is `(4096MB — 300MB) * 0.75 * (1.0 — 0.5) = 1423MB`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory boarders crossing\n",
    "\n",
    "There are situations when crossing the borders of Execution and Storage memory segments are possible.\n",
    "\n",
    "1. Storage memory can use Execution memory if there are no blocks in use at the moment.\n",
    "2. Execution memory can use Storage memory if there are unused blocks which could be evicted.\n",
    "3. Execution memory can evict Storage blocks if Storage memory has blocks in Execution region and Execution needs more memory.\n",
    "4. If Storage needs more memory and Execution uses blocks for storage, it cannot evict Execution blocks. It will wait until execution releases blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "Spark uses serialization mechanism to convert Java objects into bytes, for example to save storage space.\n",
    "\n",
    "Serializaiton formats, which are either slow or heavy, will affect performance of an application.  \n",
    "There is a tradeoff between usability and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Java Serializable\n",
    "\n",
    "Default is Spark.\n",
    "\n",
    "Built-in Java objects serialization mechanism. Allows to serialize any object which implements `java.io.Serializable`.\n",
    "\n",
    "Drawbacks are - heavy and slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kryo Serialization\n",
    "\n",
    "[Kryo](https://github.com/EsotericSoftware/kryo) is a library, outside of the JDK.\n",
    "\n",
    "Switch Spark to Kryo:  \n",
    "`conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")`\n",
    "\n",
    "\n",
    "Benefits:  \n",
    "Is faster, often 10x compact then Java Serialization.\n",
    "\n",
    "Drawbacks:  \n",
    "It requires registration.\n",
    "\n",
    "If you want an object to be serialized by Kryo, you need to register its class.  \n",
    "`conf.registerKryoClasses(Array(classOf[Class1]))`  \n",
    "\n",
    "If you don't register, Kryo will have to keep the class metadata with each object, and it is much less efficient.\n",
    "You can configure mandatory registration:  \n",
    "`spark.kryo.registrationRequired=true`\n",
    "\n",
    "When serializing large objects, might want to tweak the config parameter:  \n",
    "`spark.kryoserializer.buffer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serizalization advice\n",
    "\n",
    "Try to always use Kryo, because serialization affects two major aspects:\n",
    "1. Shuffle operations are hugely dependent on the size and speed of the serialization.\n",
    "1. Caching depends on serialization specially when caching to disk or when data spills over from memory to disk and also when MEMORY_ONLY_SER storage level is set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garbage Collection\n",
    "\n",
    "JVM GC is a mechanism for cleaning up unused memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JVM Memory Regions\n",
    "\n",
    "![jvm_memory](images/jvm_memory.png)\n",
    "\n",
    "Primarily Heap is divided in two parts: Young Generation and Old Generation.\n",
    "\n",
    "-Xms\t\t\t\tFor setting the initial heap size when JVM starts  \n",
    "-Xmx\t\t\t\tFor setting the maximum heap size.  \n",
    "-Xmn\t\t\t\tFor setting the size of the Young Generation, rest of the space goes for Old Generation.  \n",
    "-XX:PermGen\t\t\tFor setting the initial size of the Permanent Generation memory  \n",
    "-XX:MaxPermGen\t\tFor setting the maximum size of Perm Gen  \n",
    "-XX:SurvivorRatio\tFor providing ratio of Eden space and Survivor Space, for example if Young Generation size is 10m and VM switch is -XX:SurvivorRatio=2 then 5m will be reserved for Eden Space and 2.5m each for both the Survivor spaces. The default value is 8.  \n",
    "-XX:NewRatio\t\tFor providing ratio of old/new generation sizes. The default value is 2. \n",
    "\n",
    "Configure `spark.executor.extraJavaOptions` with `-XX:+PrintGCDetails` and add specific memory configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GC Types\n",
    "\n",
    "**Serial GC** (-XX:+UseSerialGC): Serial GC uses the simple mark-sweep-compact approach for young and old generations garbage collection i.e Minor and Major GC.Serial GC is useful in client machines such as our simple stand-alone applications and machines with smaller CPU. It is good for small applications with low memory footprint.\n",
    "\n",
    "**Parallel GC** (-XX:+UseParallelGC): Parallel GC is same as Serial GC except that is spawns N threads for young generation garbage collection where N is the number of CPU cores in the system. We can control the number of threads using -XX:ParallelGCThreads=n JVM option.\n",
    "\n",
    "**Parallel Old GC** (-XX:+UseParallelOldGC): This is same as Parallel GC except that it uses multiple threads for both Young Generation and Old Generation garbage collection.\n",
    "\n",
    "**Concurrent Mark Sweep** (CMS) Collector (-XX:+UseConcMarkSweepGC): CMS Collector is also referred as concurrent low pause collector. It does the garbage collection for the Old generation. CMS collector tries to minimize the pauses due to garbage collection by doing most of the garbage collection work concurrently with the application threads.\n",
    "\n",
    "**G1 Garbage Collector** (-XX:+UseG1GC): G1 collector is a parallel, concurrent, and incrementally compacting low-pause garbage collector. Garbage First Collector doesn’t work like other collectors and there is no concept of Young and Old generation space. It divides the heap space into multiple equal-sized heap regions. When a garbage collection is invoked, it first collects the region with lesser live data, hence “Garbage First”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profilers\n",
    "\n",
    "Profilers allow to monitor execution metrics of a JVM.\n",
    "\n",
    "##### jstat\n",
    "Command line tool, ships with JDK, suitable for in place monitoring on a node.\n",
    "\n",
    "```\n",
    "ps -eaf |grep MyJavaApp\n",
    "\n",
    "jstat -gc 5324 1000\n",
    "```\n",
    "\n",
    "and you will get metrics like:\n",
    "\n",
    "```\n",
    "S0C    S1C    S0U    S1U      EC       EU        OC         OU       PC     PU    YGC     YGCT    FGC    FGCT     GCT\n",
    "1024.0 1024.0  0.0    0.0    8192.0   7933.3   42108.0    23401.3   20480.0 19990.9    157    0.274  40      1.381    1.654\n",
    "1024.0 1024.0  0.0    0.0    8192.0   8026.5   42108.0    23401.3   20480.0 19990.9    157    0.274  40      1.381    1.654\n",
    "```\n",
    "\n",
    "##### Java VisualVM\n",
    "\n",
    "Also ships with JDK and allows to bind a UI to local or remote Java process. \n",
    "\n",
    "![jvisualvm-jmx-connection](images/jvisualvm-jmx-connection.png)\n",
    "\n",
    "\n",
    "Observe the state the JVM.\n",
    "\n",
    "![jvisualvm-monitoring](images/jvisualvm-monitoring.png)\n",
    "\n",
    "\n",
    "Analyze memory allocations.\n",
    "\n",
    "![jvisualvm-profiler-memory](images/jvisualvm-profiler-memory.png)\n",
    "\n",
    "\n",
    "Perform CPU snapshots.\n",
    "\n",
    "![jvisualvm-profiler-cpu-snapshot](images/jvisualvm-profiler-cpu-snapshot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this chapter we learned about:\n",
    "- How Spark memory is organized, fractions and regions\n",
    "- What is a Serialization Process and which serializers are available\n",
    "- When we need to think about the JVM Garbage Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What different parts of memory strucuture you leart and what is the role of each of them? \n",
    "2. Let's run try to see the performance of the job with different memory settings. Try to run the queries, explore execution plan and explain it.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import functions as F, SQLContext, SparkSession, Window\n",
    "from pyspark.sql.types import*\n",
    "from random import randint\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"explore-data\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.eventLog.enabled\", \"true\")\n",
    "         .config(\"spark.eventLog.dir\", \"/opt/workspace/history\")\n",
    "         .config(\"spark.memory.fraction\", \"0.75\")  #0.75 # 0.5  #0.2\n",
    "         .config(\"spark.memory.storageFraction\", \"0.5\")  #0.5 # 0.1 #0.1\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "meteo_data_file = \"data/meteo-data/parquet\"\n",
    "meteo_df = spark.read.parquet(meteo_data_file)\n",
    "observation_type_file = \"data/meteo-data/observation_type.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('observation_type', StringType(), True),\n",
    "    StructField('description', StringType(), True)\n",
    "])\n",
    "\n",
    "observation_type_df = (spark.read\n",
    "               .schema(schema)\n",
    "               .option(\"header\", \"false\")\n",
    "               .csv(observation_type_file)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = (meteo_df\n",
    "         .where(\"observation_type = 'SNOW'\")\n",
    "         .where(\"yyyy = 2010\")\n",
    "      ).cache()\n",
    "df2 = (meteo_df\n",
    "         .where(\"observation_type = 'TAVG'\")\n",
    "         .where(\"yyyy = 2010\")\n",
    "      ).cache()\n",
    "\n",
    "df1.join(df2, [\"station_identifier\",\"date\"], \"left\").cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How does spark.memory.fraction impact performance of the job? \n",
    "2. How does park.memory.storageFraction impact performance of the job? \n",
    "3. When we need to change this settings and what is the ideal value? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
